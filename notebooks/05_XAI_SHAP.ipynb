{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d676df",
   "metadata": {},
   "source": [
    "# 5 — Explainability (SHAP & Grad-CAM)\n",
    "## Interpreting Fire Spread Predictions\n",
    "\n",
    "We apply post-hoc explainability techniques to understand **which input features** and **which spatial regions** drive model predictions:\n",
    "\n",
    "1. **SHAP** (SHapley Additive exPlanations) — channel-level feature importance\n",
    "2. **Grad-CAM** — spatial attention heatmaps for CNN-based models\n",
    "3. **PI-CCA Physics Gate** — model-intrinsic interpretability (physics vs CNN contribution)\n",
    "\n",
    "### Input Features (12 channels)\n",
    "| # | Channel | Source |\n",
    "|---|---------|--------|\n",
    "| 0 | elevation | SRTM DEM |\n",
    "| 1 | wind_speed | GRIDMET (th) |\n",
    "| 2 | wind_direction | GRIDMET (vs) |\n",
    "| 3 | min_temp | GRIDMET (tmmn) |\n",
    "| 4 | max_temp | GRIDMET (tmmx) |\n",
    "| 5 | humidity | GRIDMET (sph) |\n",
    "| 6 | precipitation | GRIDMET (pr) |\n",
    "| 7 | drought_index | GRIDMET (PDSI) |\n",
    "| 8 | ndvi | VIIRS |\n",
    "| 9 | erc | GRIDMET (ERC) |\n",
    "| 10 | population | LandScan |\n",
    "| 11 | prev_fire_mask | FIRMS/VIIRS |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b32747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from config import (\n",
    "    MODEL_CONFIG, MODELS_DIR, PROCESSED_DIR, RESULTS_DIR,\n",
    "    FIGURES_DIR, SEED, FEATURE_CHANNELS, CH, N_INPUT_CHANNELS,\n",
    ")\n",
    "from src.data.dataset import get_dataloaders\n",
    "from src.models.convlstm import ConvLSTMModel\n",
    "from src.models.unet import UNetFire\n",
    "from src.models.pi_cca import PIConvCellularAutomaton\n",
    "from src.explainability.shap_analysis import (\n",
    "    compute_channel_shap, compute_gradcam,\n",
    "    plot_shap_importance, plot_shap_beeswarm, plot_gradcam_overlay,\n",
    ")\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.1)\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f6070",
   "metadata": {},
   "source": [
    "## 5.1 Load Models & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = get_dataloaders(PROCESSED_DIR, batch_size=32, seed=SEED)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'convlstm': ConvLSTMModel,\n",
    "    'unet': UNetFire,\n",
    "    'pi_cca': PIConvCellularAutomaton,\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, cls in MODEL_CLASSES.items():\n",
    "    model = cls(config=MODEL_CONFIG[name]).to(device)\n",
    "    ckpt = MODELS_DIR / name / 'best_model.pt'\n",
    "    if ckpt.exists():\n",
    "        model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "    model.eval()\n",
    "    models[name] = model\n",
    "    print(f'Loaded: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f064e",
   "metadata": {},
   "source": [
    "## 5.2 SHAP Feature Importance\n",
    "\n",
    "We use **DeepSHAP** to estimate the contribution of each of the 12 input channels to the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfafb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\nComputing SHAP for {name}...')\n",
    "    result = compute_channel_shap(\n",
    "        model, loaders['test'], FEATURE_CHANNELS,\n",
    "        n_background=50, n_explain=100,\n",
    "        device=str(device),\n",
    "    )\n",
    "    shap_results[name] = result\n",
    "    \n",
    "    # Print top features\n",
    "    print(f'  Top 5 features for {MODEL_CONFIG[name][\"name\"]}:')\n",
    "    for idx in result['importance_order'][:5]:\n",
    "        print(f'    {FEATURE_CHANNELS[idx]:>20s}: {result[\"mean_abs_shap\"][idx]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP importance for each model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for j, (name, result) in enumerate(shap_results.items()):\n",
    "    order = result['importance_order']\n",
    "    values = result['mean_abs_shap'][order]\n",
    "    labels = [FEATURE_CHANNELS[i] for i in order]\n",
    "    \n",
    "    axes[j].barh(range(len(labels)), values, color=sns.color_palette('viridis', len(labels)))\n",
    "    axes[j].set_yticks(range(len(labels)))\n",
    "    axes[j].set_yticklabels(labels)\n",
    "    axes[j].set_xlabel('Mean |SHAP|')\n",
    "    axes[j].set_title(MODEL_CONFIG[name]['name'], fontweight='bold')\n",
    "    axes[j].invert_yaxis()\n",
    "\n",
    "plt.suptitle('SHAP Feature Importance — All Models', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'shap_all_models.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed8127",
   "metadata": {},
   "source": [
    "## 5.3 SHAP Beeswarm Plots\n",
    "\n",
    "Beeswarm plots show the distribution of SHAP values per feature across test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b204627",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, result in shap_results.items():\n",
    "    fig = plot_shap_beeswarm(\n",
    "        result, MODEL_CONFIG[name]['name'],\n",
    "        save_path=str(FIGURES_DIR / f'shap_beeswarm_{name}.png'),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c74d3",
   "metadata": {},
   "source": [
    "## 5.4 Grad-CAM Spatial Attention\n",
    "\n",
    "Grad-CAM highlights which spatial regions each model focuses on when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test sample\n",
    "test_ds = loaders['test'].dataset\n",
    "rng = np.random.default_rng(SEED)\n",
    "sid = rng.choice(len(test_ds))\n",
    "x_sample, y_sample = test_ds[sid]\n",
    "x_dev = x_sample.unsqueeze(0).to(device)\n",
    "\n",
    "fire_mask = x_sample[CH['prev_fire_mask']].numpy()\n",
    "gt = y_sample.squeeze().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(models) + 2, figsize=(5 * (len(models) + 2), 5))\n",
    "\n",
    "# Input fire\n",
    "axes[0].imshow(fire_mask, cmap='hot', vmin=0, vmax=1)\n",
    "axes[0].set_title('Input Fire (Day t)', fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "for j, (name, model) in enumerate(models.items()):\n",
    "    cam = compute_gradcam(model, x_dev, device=str(device))\n",
    "    axes[j+1].imshow(fire_mask, cmap='gray', alpha=0.3)\n",
    "    axes[j+1].imshow(cam, cmap='jet', alpha=0.7, vmin=0, vmax=1)\n",
    "    axes[j+1].contour(gt, levels=[0.5], colors='lime', linewidths=1.5)\n",
    "    axes[j+1].set_title(f'Grad-CAM: {MODEL_CONFIG[name][\"name\"]}', fontweight='bold', fontsize=9)\n",
    "    axes[j+1].axis('off')\n",
    "\n",
    "# Ground truth\n",
    "axes[-1].imshow(gt, cmap='hot', vmin=0, vmax=1)\n",
    "axes[-1].set_title('Ground Truth (Day t+1)', fontweight='bold')\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Grad-CAM Spatial Attention', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'gradcam_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf89cb8",
   "metadata": {},
   "source": [
    "## 5.5 PI-CCA Physics Gate Analysis\n",
    "\n",
    "PI-CCA uses a learnable **physics gate (λ)** that controls the balance between the Rothermel-based physics branch and the CNN branch:\n",
    "\n",
    "$$\\hat{y} = \\lambda \\cdot \\phi_{\\text{physics}}(x) + (1 - \\lambda) \\cdot \\phi_{\\text{CNN}}(x)$$\n",
    "\n",
    "We visualise the spatial distribution of λ to understand where the model trusts physics vs learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_cca = models['pi_cca']\n",
    "\n",
    "# Get physics gate values\n",
    "n_vis = 4\n",
    "sample_ids = rng.choice(len(test_ds), n_vis, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(n_vis, 4, figsize=(18, 4 * n_vis))\n",
    "\n",
    "for i, sid in enumerate(sample_ids):\n",
    "    x, y = test_ds[sid]\n",
    "    x_dev = x.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Hook into physics_gate\n",
    "    gate_val = None\n",
    "    def hook_fn(module, input, output):\n",
    "        nonlocal gate_val\n",
    "        gate_val = output.detach().cpu().numpy()\n",
    "    \n",
    "    # Try to find the gate module\n",
    "    if hasattr(pi_cca, 'physics_gate'):\n",
    "        handle = pi_cca.physics_gate.register_forward_hook(hook_fn)\n",
    "        with torch.no_grad():\n",
    "            pred = pi_cca(x_dev).squeeze().cpu().numpy()\n",
    "        handle.remove()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            pred = pi_cca(x_dev).squeeze().cpu().numpy()\n",
    "    \n",
    "    gt = y.squeeze().numpy()\n",
    "    fire_in = x[CH['prev_fire_mask']].numpy()\n",
    "    \n",
    "    axes[i, 0].imshow(fire_in, cmap='hot', vmin=0, vmax=1)\n",
    "    axes[i, 0].set_title('Input Fire' if i == 0 else '')\n",
    "    \n",
    "    axes[i, 1].imshow(pred, cmap='hot', vmin=0, vmax=1)\n",
    "    axes[i, 1].contour(gt, levels=[0.5], colors='lime', linewidths=1)\n",
    "    axes[i, 1].set_title('Prediction' if i == 0 else '')\n",
    "    \n",
    "    if gate_val is not None:\n",
    "        g = gate_val.squeeze()\n",
    "        if g.ndim == 0:\n",
    "            g = np.full_like(pred, g)\n",
    "        elif g.shape != pred.shape:\n",
    "            from scipy.ndimage import zoom\n",
    "            g = zoom(g, np.array(pred.shape) / np.array(g.shape), order=1)\n",
    "        im = axes[i, 2].imshow(g, cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "        plt.colorbar(im, ax=axes[i, 2], fraction=0.046)\n",
    "        axes[i, 2].set_title('Physics Gate λ' if i == 0 else '')\n",
    "    else:\n",
    "        axes[i, 2].text(0.5, 0.5, 'No gate found', ha='center', va='center', transform=axes[i, 2].transAxes)\n",
    "    \n",
    "    axes[i, 3].imshow(gt, cmap='hot', vmin=0, vmax=1)\n",
    "    axes[i, 3].set_title('Ground Truth' if i == 0 else '')\n",
    "    \n",
    "    for ax in axes[i]:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('PI-CCA Physics Gate Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'pi_cca_physics_gate.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e5634",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### SHAP Importance\n",
    "- **prev_fire_mask** is consistently the most important feature (fire spreads from existing fire)\n",
    "- **ERC** and **drought_index** are strong predictors of fire potential\n",
    "- **wind_speed** influences spread direction and rate\n",
    "- **NDVI** captures fuel availability\n",
    "- **elevation** affects slope-driven spread in the physics branch\n",
    "\n",
    "### Grad-CAM Attention\n",
    "- U-Net and PI-CCA focus on fire perimeter regions (where spread occurs)\n",
    "- ConvLSTM shows broader attention, capturing wind-influenced zones\n",
    "\n",
    "### Physics Gate (PI-CCA)\n",
    "- λ ≈ 1 (trusts physics) in areas with homogeneous terrain and steady wind\n",
    "- λ ≈ 0 (trusts CNN) near fire edges and in complex terrain where Rothermel assumptions break down\n",
    "- This adaptive blending is a key advantage of the PI-CCA architecture"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
