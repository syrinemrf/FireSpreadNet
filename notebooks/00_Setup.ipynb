{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9e1900",
   "metadata": {},
   "source": [
    "# 00 — Setup & Conversion\n",
    "## FireSpreadNet · Next Day Wildfire Spread\n",
    "\n",
    "**Ce notebook est autonome** : il ne dépend d'aucun script externe.  \n",
    "Il suffit de placer ce notebook dans le même dossier que les autres notebooks,  \n",
    "et d'avoir les fichiers `.tfrecord` accessibles quelque part sur le disque.\n",
    "\n",
    "### Ce que fait ce notebook :\n",
    "1. Détecte automatiquement le dossier contenant les fichiers `.tfrecord`\n",
    "2. Installe les dépendances manquantes si nécessaire\n",
    "3. Convertit les TFRecords → numpy `.npz` (dans `processed/`)\n",
    "4. Sauvegarde un fichier `setup_config.json` lu par tous les autres notebooks\n",
    "\n",
    "### Usage :\n",
    "- Exécute toutes les cellules de haut en bas une seule fois\n",
    "- Les autres notebooks (`01_EDA`, `02_Preprocessing`, etc.) peuvent ensuite être lancés directement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7d907",
   "metadata": {},
   "source": [
    "## Étape 1 — Dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, subprocess, sys\n",
    "\n",
    "def ensure(pkg, import_name=None):\n",
    "    import_name = import_name or pkg\n",
    "    if importlib.util.find_spec(import_name) is None:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "    else:\n",
    "        print(f\"  {pkg} — OK\")\n",
    "\n",
    "print(\"Checking dependencies...\")\n",
    "ensure(\"numpy\")\n",
    "ensure(\"tqdm\")\n",
    "ensure(\"matplotlib\")\n",
    "ensure(\"seaborn\")\n",
    "ensure(\"pandas\")\n",
    "\n",
    "# TFRecord parsing backend — try tensorflow first, then lightweight tfrecord\n",
    "_BACKEND = None\n",
    "if importlib.util.find_spec(\"tensorflow\") is not None:\n",
    "    _BACKEND = \"tensorflow\"\n",
    "    print(\"  tensorflow — OK (TFRecord backend)\")\n",
    "elif importlib.util.find_spec(\"tfrecord\") is not None:\n",
    "    _BACKEND = \"tfrecord\"\n",
    "    print(\"  tfrecord — OK (TFRecord backend)\")\n",
    "else:\n",
    "    print(\"  Installing tfrecord (lightweight, no TF needed)...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tfrecord\", \"-q\"])\n",
    "    _BACKEND = \"tfrecord\"\n",
    "\n",
    "print(f\"\\nTFRecord backend: {_BACKEND}\")\n",
    "print(\"All dependencies OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d8710",
   "metadata": {},
   "source": [
    "## Étape 2 — Détection automatique des chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ── Notebook directory (works wherever the notebooks are placed) ──────────────\n",
    "NB_DIR = Path().resolve()   # current working directory when running the notebook\n",
    "\n",
    "# ── Locate TFRecord files (search parent directories and common sub-paths) ────\n",
    "TFRECORD_SEARCH_ROOTS = [\n",
    "    NB_DIR,\n",
    "    NB_DIR.parent,\n",
    "    NB_DIR.parent / \"data\",\n",
    "    NB_DIR.parent / \"data\" / \"raw\",\n",
    "    NB_DIR.parent / \"data\" / \"raw\" / \"ml_tracks\" / \"a.fire_danger\",\n",
    "    NB_DIR / \"data\",\n",
    "    NB_DIR / \"data\" / \"raw\",\n",
    "    NB_DIR / \"data\" / \"raw\" / \"ml_tracks\" / \"a.fire_danger\",\n",
    "]\n",
    "\n",
    "RAW_DATA_DIR = None\n",
    "for root in TFRECORD_SEARCH_ROOTS:\n",
    "    if root.exists():\n",
    "        tfr_files = list(root.rglob(\"*.tfrecord\")) + list(root.rglob(\"*.tfrecord*\"))\n",
    "        if tfr_files:\n",
    "            # Use the directory containing the first .tfrecord found\n",
    "            RAW_DATA_DIR = tfr_files[0].parent\n",
    "            break\n",
    "\n",
    "if RAW_DATA_DIR is None:\n",
    "    print(\"⚠️  TFRecord files not found automatically.\")\n",
    "    print(\"    Set RAW_DATA_DIR manually below:\")\n",
    "    RAW_DATA_DIR = NB_DIR.parent / \"data\" / \"raw\" / \"ml_tracks\" / \"a.fire_danger\"\n",
    "    print(f\"    Using default: {RAW_DATA_DIR}\")\n",
    "else:\n",
    "    tfr_count = len(list(RAW_DATA_DIR.glob(\"*.tfrecord*\")))\n",
    "    print(f\"✅ Found TFRecord data in: {RAW_DATA_DIR}\")\n",
    "    print(f\"   {tfr_count} .tfrecord file(s) detected\")\n",
    "\n",
    "# ── Output directories ────────────────────────────────────────────────────────\n",
    "PROCESSED_DIR = NB_DIR.parent / \"data\" / \"processed\"\n",
    "FIGURES_DIR   = NB_DIR.parent / \"results\" / \"figures\"\n",
    "MODELS_DIR    = NB_DIR.parent / \"saved_models\"\n",
    "\n",
    "for d in [PROCESSED_DIR, FIGURES_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nProcessed output : {PROCESSED_DIR}\")\n",
    "print(f\"Figures output   : {FIGURES_DIR}\")\n",
    "print(f\"Models output    : {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3322a6",
   "metadata": {},
   "source": [
    "## Étape 3 — Configuration (embarquée, sans import externe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION — Next Day Wildfire Spread (Huot et al., 2022)\n",
    "# Entièrement embarquée — aucun import de config.py nécessaire\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "\n",
    "GRID_SIZE = 64          # taille du patch spatial (pixels)\n",
    "CELL_SIZE_KM = 1.0      # résolution ~1 km/pixel (MODIS)\n",
    "TIMESTEP_H = 24         # pas de temps : 1 jour\n",
    "\n",
    "# Noms lisibles des 12 canaux d'entrée\n",
    "FEATURE_CHANNELS = [\n",
    "    \"elevation\",        # SRTM — altitude (m)\n",
    "    \"wind_speed\",       # GRIDMET 'th' — vitesse du vent (m/s)\n",
    "    \"wind_direction\",   # GRIDMET 'vs' — direction du vent (°)\n",
    "    \"min_temp\",         # GRIDMET 'tmmn' — temp. min. (K)\n",
    "    \"max_temp\",         # GRIDMET 'tmmx' — temp. max. (K)\n",
    "    \"humidity\",         # GRIDMET 'sph' — humidité spécifique (kg/kg)\n",
    "    \"precipitation\",    # GRIDMET 'pr' — précipitations (mm)\n",
    "    \"drought_index\",    # GRIDMET 'PDSI' — indice de sécheresse\n",
    "    \"ndvi\",             # VIIRS NDVI — végétation (−1 à 1)\n",
    "    \"erc\",              # GRIDMET 'ERC' — Energy Release Component\n",
    "    \"population\",       # LandScan — densité de population (pers/km²)\n",
    "    \"prev_fire_mask\",   # FIRMS/VIIRS — masque feu J-1 (binaire)\n",
    "]\n",
    "N_INPUT_CHANNELS = len(FEATURE_CHANNELS)  # 12\n",
    "\n",
    "# Clés TFRecord réelles (noms dans les fichiers .tfrecord)\n",
    "TFRECORD_INPUT_KEYS = [\n",
    "    \"elevation\", \"th\", \"vs\", \"tmmn\", \"tmmx\",\n",
    "    \"sph\", \"pr\", \"PDSI\", \"NDVI\", \"ERC\",\n",
    "    \"population\", \"PrevFireMask\",\n",
    "]\n",
    "TFRECORD_TARGET_KEY = \"FireMask\"\n",
    "\n",
    "# Indice de chaque canal (pour les modèles physiques)\n",
    "CH = {name: i for i, name in enumerate(FEATURE_CHANNELS)}\n",
    "\n",
    "N_PIXELS = GRID_SIZE * GRID_SIZE  # 4096\n",
    "\n",
    "print(f\"Canaux d'entrée ({N_INPUT_CHANNELS}) :\")\n",
    "for i, name in enumerate(FEATURE_CHANNELS):\n",
    "    print(f\"  [{i:2d}] {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672e5b9",
   "metadata": {},
   "source": [
    "## Étape 4 — Conversion TFRecord → NumPy (.npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea518c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ALL_FEATURES = TFRECORD_INPUT_KEYS + [TFRECORD_TARGET_KEY]\n",
    "\n",
    "# ── Helpers ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "def find_tfrecords(root: Path, split_tag: str):\n",
    "    \"\"\"Trouve les fichiers .tfrecord pour un split donné.\"\"\"\n",
    "    patterns = [f\"*{split_tag}*.tfrecord\", f\"*{split_tag}*.tfrecord*\"]\n",
    "    files = []\n",
    "    for p in patterns:\n",
    "        files += sorted(root.rglob(p))\n",
    "    # Déduplique, retire les .index\n",
    "    seen, unique = set(), []\n",
    "    for f in files:\n",
    "        if f not in seen and not f.name.endswith(\".index\"):\n",
    "            seen.add(f); unique.append(f)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def parse_tf(tfrecord_files, max_samples=None):\n",
    "    \"\"\"Parse avec TensorFlow.\"\"\"\n",
    "    import tensorflow as tf\n",
    "    desc = {f: tf.io.FixedLenFeature([N_PIXELS], tf.float32) for f in ALL_FEATURES}\n",
    "    X_list, Y_list, count = [], [], 0\n",
    "    for fpath in tqdm(tfrecord_files, desc=\"TF backend\"):\n",
    "        for raw in tf.data.TFRecordDataset(str(fpath)):\n",
    "            p = tf.io.parse_single_example(raw, desc)\n",
    "            x = np.stack([p[f].numpy().reshape(GRID_SIZE, GRID_SIZE)\n",
    "                          for f in TFRECORD_INPUT_KEYS], axis=0).astype(np.float32)\n",
    "            y = p[TFRECORD_TARGET_KEY].numpy().reshape(1, GRID_SIZE, GRID_SIZE).astype(np.float32)\n",
    "            if x[-1].max() == 0 and y.max() == 0: continue\n",
    "            X_list.append(x); Y_list.append(y); count += 1\n",
    "            if max_samples and count >= max_samples: break\n",
    "        if max_samples and count >= max_samples: break\n",
    "    return (np.stack(X_list), np.stack(Y_list)) if X_list else (np.empty((0,)), np.empty((0,)))\n",
    "\n",
    "\n",
    "def parse_tfrecord_pkg(tfrecord_files, max_samples=None):\n",
    "    \"\"\"Parse avec le package léger tfrecord (pas de TF).\"\"\"\n",
    "    import tfrecord as tfr\n",
    "    desc = {f: \"float\" for f in ALL_FEATURES}\n",
    "    X_list, Y_list, count = [], [], 0\n",
    "    for fpath in tqdm(tfrecord_files, desc=\"tfrecord pkg\"):\n",
    "        for rec in tfr.tfrecord_loader(str(fpath), index_path=None, description=desc):\n",
    "            x = np.stack([np.array(rec[f], dtype=np.float32).reshape(GRID_SIZE, GRID_SIZE)\n",
    "                          for f in TFRECORD_INPUT_KEYS], axis=0)\n",
    "            y = np.array(rec[TFRECORD_TARGET_KEY], dtype=np.float32).reshape(1, GRID_SIZE, GRID_SIZE)\n",
    "            if x[-1].max() == 0 and y.max() == 0: continue\n",
    "            X_list.append(x); Y_list.append(y); count += 1\n",
    "            if max_samples and count >= max_samples: break\n",
    "        if max_samples and count >= max_samples: break\n",
    "    return (np.stack(X_list), np.stack(Y_list)) if X_list else (np.empty((0,)), np.empty((0,)))\n",
    "\n",
    "\n",
    "parse_fn = parse_tf if _BACKEND == \"tensorflow\" else parse_tfrecord_pkg\n",
    "\n",
    "# ── Conversion ────────────────────────────────────────────────────────────────\n",
    "\n",
    "# tag des fichiers par split : train=train, val=eval, test=test\n",
    "SPLIT_TAGS = {\"train\": \"train\", \"val\": \"eval\", \"test\": \"test\"}\n",
    "\n",
    "# ⚠️  Optionnel : limiter le nombre d'échantillons pour un test rapide\n",
    "MAX_SAMPLES = None   # Mettre ex. 2000 pour un test rapide, None = tout\n",
    "\n",
    "split_stats = {}\n",
    "\n",
    "for split, tag in SPLIT_TAGS.items():\n",
    "    out_path = PROCESSED_DIR / f\"{split}.npz\"\n",
    "    if out_path.exists():\n",
    "        data = np.load(out_path)\n",
    "        print(f\"  {split}: déjà converti — {data['X'].shape[0]} échantillons ({out_path.name})\")\n",
    "        split_stats[split] = int(data['X'].shape[0])\n",
    "        continue\n",
    "\n",
    "    files = find_tfrecords(RAW_DATA_DIR, tag)\n",
    "    if not files:\n",
    "        print(f\"  ⚠️  {split} ({tag}): aucun fichier trouvé dans {RAW_DATA_DIR}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n  Conversion {split} ({len(files)} fichier(s))...\")\n",
    "    X, Y = parse_fn(files, MAX_SAMPLES)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        print(f\"  ⚠️  Aucun échantillon valide pour {split}\")\n",
    "        continue\n",
    "\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    Y = (np.nan_to_num(Y, nan=0.0) > 0).astype(np.float32)\n",
    "\n",
    "    np.savez_compressed(out_path, X=X, Y=Y)\n",
    "    split_stats[split] = len(X)\n",
    "    print(f\"  ✅ {split}: {len(X)} échantillons → {out_path.name}\")\n",
    "    print(f\"     X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "    print(f\"     Taux pixels feu: {Y.mean():.4f}\")\n",
    "\n",
    "print(\"\\nConversion terminée !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c186d3",
   "metadata": {},
   "source": [
    "## Étape 5 — Statistiques de normalisation (à partir du train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f963a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "norm_stats = {}\n",
    "train_npz = PROCESSED_DIR / \"train.npz\"\n",
    "\n",
    "if train_npz.exists():\n",
    "    print(\"Calcul des statistiques par canal (train set)...\")\n",
    "    data = np.load(train_npz)\n",
    "    X_train = data['X']   # (N, C, H, W)\n",
    "    for i, name in enumerate(FEATURE_CHANNELS):\n",
    "        ch = X_train[:, i].astype(np.float64)\n",
    "        norm_stats[name] = {\n",
    "            \"mean\": float(np.nanmean(ch)),\n",
    "            \"std\":  float(np.nanstd(ch)) + 1e-8,\n",
    "        }\n",
    "    print(f\"  {len(norm_stats)} canaux traités\\n\")\n",
    "    print(f\"  {'Canal':<20} {'Moyenne':>12} {'Écart-type':>12}\")\n",
    "    print(f\"  {'-'*44}\")\n",
    "    for name, s in norm_stats.items():\n",
    "        print(f\"  {name:<20} {s['mean']:>12.4f} {s['std']:>12.4f}\")\n",
    "else:\n",
    "    print(\"⚠️  train.npz introuvable — statistiques par défaut utilisées.\")\n",
    "    # Valeurs approximatives issues de la littérature (Huot et al., 2022)\n",
    "    norm_stats = {\n",
    "        \"elevation\":      {\"mean\": 1200.0, \"std\": 800.0},\n",
    "        \"wind_speed\":     {\"mean\": 3.5,    \"std\": 1.5},\n",
    "        \"wind_direction\": {\"mean\": 200.0,  \"std\": 80.0},\n",
    "        \"min_temp\":       {\"mean\": 285.0,  \"std\": 8.0},\n",
    "        \"max_temp\":       {\"mean\": 305.0,  \"std\": 8.0},\n",
    "        \"humidity\":       {\"mean\": 0.005,  \"std\": 0.003},\n",
    "        \"precipitation\":  {\"mean\": 1.0,    \"std\": 5.0},\n",
    "        \"drought_index\":  {\"mean\": 0.0,    \"std\": 3.0},\n",
    "        \"ndvi\":           {\"mean\": 0.3,    \"std\": 0.2},\n",
    "        \"erc\":            {\"mean\": 40.0,   \"std\": 25.0},\n",
    "        \"population\":     {\"mean\": 50.0,   \"std\": 200.0},\n",
    "        \"prev_fire_mask\": {\"mean\": 0.0,    \"std\": 1.0},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50e3b9",
   "metadata": {},
   "source": [
    "## Étape 6 — Sauvegarde de la configuration (setup_config.json)\n",
    "Ce fichier est chargé par tous les autres notebooks au démarrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d451de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "setup_config = {\n",
    "    \"PROCESSED_DIR\":     str(PROCESSED_DIR),\n",
    "    \"FIGURES_DIR\":       str(FIGURES_DIR),\n",
    "    \"MODELS_DIR\":        str(MODELS_DIR),\n",
    "    \"RAW_DATA_DIR\":      str(RAW_DATA_DIR),\n",
    "    \"GRID_SIZE\":         GRID_SIZE,\n",
    "    \"CELL_SIZE_KM\":      CELL_SIZE_KM,\n",
    "    \"TIMESTEP_H\":        TIMESTEP_H,\n",
    "    \"FEATURE_CHANNELS\":  FEATURE_CHANNELS,\n",
    "    \"N_INPUT_CHANNELS\":  N_INPUT_CHANNELS,\n",
    "    \"CH\":                CH,\n",
    "    \"TFRECORD_INPUT_KEYS\":  TFRECORD_INPUT_KEYS,\n",
    "    \"TFRECORD_TARGET_KEY\":  TFRECORD_TARGET_KEY,\n",
    "    \"norm_stats\":        norm_stats,\n",
    "    \"split_samples\":     split_stats,\n",
    "    \"tfrecord_backend\":  _BACKEND,\n",
    "}\n",
    "\n",
    "CONFIG_PATH = NB_DIR / \"setup_config.json\"\n",
    "with open(CONFIG_PATH, \"w\") as f:\n",
    "    json.dump(setup_config, f, indent=2)\n",
    "\n",
    "print(f\"✅  setup_config.json sauvegardé : {CONFIG_PATH}\")\n",
    "print(\"\\nLes autres notebooks peuvent le charger avec :\")\n",
    "print(\"  import json\")\n",
    "print(\"  from pathlib import Path\")\n",
    "print(\"  cfg = json.load(open(Path().resolve() / 'setup_config.json'))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232412fa",
   "metadata": {},
   "source": [
    "## Étape 7 — Résumé & vérification finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62126d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"  RÉSUMÉ DU SETUP — FireSpreadNet\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "checks = {\n",
    "    \"Données brutes (TFRecord)\": RAW_DATA_DIR,\n",
    "    \"Données traitées (npz)\":    PROCESSED_DIR,\n",
    "    \"Figures\":                    FIGURES_DIR,\n",
    "    \"Modèles\":                    MODELS_DIR,\n",
    "    \"Config (setup_config.json)\": NB_DIR / \"setup_config.json\",\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for label, path in checks.items():\n",
    "    exists = Path(path).exists()\n",
    "    status = \"✅\" if exists else \"❌\"\n",
    "    print(f\"  {status}  {label}\")\n",
    "    print(f\"       {path}\")\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "print()\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    npz = PROCESSED_DIR / f\"{split}.npz\"\n",
    "    if npz.exists():\n",
    "        d = np.load(npz)\n",
    "        n = d['X'].shape[0]\n",
    "        print(f\"  ✅  {split}.npz — {n} échantillons\")\n",
    "    else:\n",
    "        print(f\"  ❌  {split}.npz — non trouvé\")\n",
    "        all_ok = False\n",
    "\n",
    "print()\n",
    "if all_ok:\n",
    "    print(\"  ✅  Setup complet — tu peux lancer les autres notebooks !\")\n",
    "else:\n",
    "    print(\"  ⚠️  Certains fichiers sont manquants — vérifie les chemins ci-dessus.\")\n",
    "\n",
    "print(\"=\" * 55)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
